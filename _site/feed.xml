<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YSlog</title>
    <description>Deep Learning paper repo</description>
    <link>https://youngsankoh.github.io/</link>
    <atom:link href="https://youngsankoh.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 10 Sep 2019 00:49:14 +0900</pubDate>
    <lastBuildDate>Tue, 10 Sep 2019 00:49:14 +0900</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>SlowFast Networks for Video Recognition</title>
        <description>Key concept

  Fast pathway - high frame rate, motion
  Slow pathway - low frame rate, spatial semantic



Introduction

network의 motivation

  영상에서 categorical spatial semantic은 ‘천천히’ 변함
    
      ‘waving hands’에서 손(categorical semantic)이라는 identity는 변하지 않음
      따라서 categorical semantic을 파악할 때는 low frame rate로 처리해도 됨
    
  
  반면 motion은 빠르게 변할 수 있음
    
      high frame rate 필요
    
  


 two-pathway SlowFast model을 제안

참고사항)

  이 모델은 영장류 시각 시스템의 retinal ganglion cell에서도 부분적으로 영감을 얻음.
  이 cell은 ~80%의 Parvocellular(P-cell), ~15~20%의 Magnocellular(M-cell)로 구성
  M-cell은 spatial detail과 색깔에는 둔감, temporal change에는 민감(high temporal frequency로 작동) (P-cell은 그 반대)



SlowFast Networks




  Slow pathway
  Fast pathway


Slow pathway

  video clip을 처리할 수 있는 모든 conv model 사용 가능 e.g., C3D, I3D
  large temporal stride 
   : frames sampled by the Slow pathway



Fast pathway
High frame rate

  small temporal stride  



High temporal resolution features

  temporal downsampling layer 없음  항상  차원
    
      temporal fidelity 유지

    
  


Low channel capacity

  Slow pathway의 channel 수 대비 배   lightweight
    
      Fast pathway은 전체 계산량의 ~20% 차지(M-cell의 비중과 유사)
    
  
  적은 channel 수로 인해 spatial semantic을 표현하는 능력이 약함.
    
      spatial modeling ability와 temporal modeling ability 사이의 tradeoff
    
  
  spatial capacity를 약화시키는 방향의 추가적인 실험
    
      input spatial resolution 축소
      색깔 정보 제거

    
  


Lateral connections

  각 pathway에서 학습한 정보 공유
  매 stage마다 connection, ResNet을 예로 들면 pool1, res2, res3, res4 이후에..
  두 pathway가 처리하는 temporal dimension이 다르기 때문에 일치시켜주는 작업 필요
    
      Instantiations 절에서 자세히
    
  
  connection은 unidirectional (fast  slow)
    
      bidirectional도 실험해봤지만 성능면에서 차이 미미

    
  


Instantiations


Slow pathway
(위 그림 예)

  3D ResNet을 사용, 
  non-degenerate temporal conv(temporal kernel size&amp;gt;1 인 conv layer, 밑줄)를 res4, res5에만 사용
    
      초기 layer에서의 temporal conv는 정확도에 악영향
      object의 움직임이 빠르고 temporal stride가 크면, spatial receptive field가 충분히 크지 않으면 temporal receptive field 사이에는 correlation이 거의 없다고 판단

    
  


Fast pathway

    
  higher temporal resolution(green), lower channel capacity(orange)
  non-degenerate conv를 모든 block에 사용



Lateral connections

  각 pathway의 feature shape
    
      Slow pathway : 
      Fast pathway : 

    
  
  세 가지 fusion 방법
    
      Time-to-channel : reshape and transpose

      Time-strided sampling : sample one out of every  frames

      Time-strided convolution : 3D conv of  kernel with stride= 


    
  


Experiments: Kinetics Action Classification
Datasets

  Kinetics-400
  평가: top-1, top-5 classification accuracy(%), FLOPs



Training

  논문 참조



Inference

  논문 참조



Results and Analysis


Training from scratch

  [50]
  our recipe : large-scale SGD



Individual pathways

  no temporal downsampling = temporal reduction factor(t-reduce) of 1
  individual pathway의 성능은 그닥, 다만 GFLOPs를 보면 매우 light함을 알 수 있음
  두 pathway는 합쳐질 때 각자의 special expertise를 보임.(다음 실험)



SlowFast fusion

  SlowFast의 첫번째 row는 naive fusion 사용. 단순 concat
  SlowFast model은 Slow-only 보다 모두 좋은 성능을 보임
  T-conv가 젤 좋음. 따라서 default lateral connection으로 설정



Channel capacity of Fast pathway

   값에 상관 없이 모두 성능 향상.
  best는 ,  (default)
  의 경우 GFLOPs가 1.0밖에 안 늘어났지만 1.6%의 성능 향상을 보임



Weaker spatial inputs to Fast pathway

  time difference : 현재 frame - 이전 frame
  모두 Slow-only baseline보다 좋음
  특히 gray-scale의 경우 RGB만큼 좋은 성능을 보이는 동시에 FLOPs도 ~5% 가량 적음
    
      이 역시 색깔에 둔감한 M-cell과 비슷

    
  


vs. Slow+Slow

  2-Slow ens. : 따로 training된 두 개의 Slow-only 모델을 앙상블
 0.5% 성능 향상, 계산량 x2
  SlowSlow : Fast pathway를 Slow pathway로 교체
 overfitting으로 고통받음..



Various SlowFast Instantiations

  
    
       
       
      두 경우 모두 약간의 FLOPs 증가로 성능 향상
    
  
  나머지는 정확도와 FLOPs 사이의 tradeoff 양상을 보임
  이 tradeoff는 더 최적화될 여지가 남아있으며, 최적의 tradeoff는 application-dependent함



Advanced backbones

  ResNet-101 모델과 non-local(NL) version에 대해 추가 실험
    
      NL은 Slow pathway에만 추가
      R101+NL에서 NL은 res4에만 추가
    
  
  advanced backbones은 SlowFast concept과는 독립적(orthogonal)
    
       추가 성능 향상

    
  


Comparison with state-of-the-art results



  각 논문마다 inference strategy가 다르기 때문에 inference time에 “view”(temporal clip with spatial crop) 개념 도입
    
      본 논문 예) 10 clips each with 3 spatial crops  30 views
    
  
  기존 모델(w/wo ImageNet pre-training)보다 성능 좋음. 동시에 inference-time cost도 낮음.
    
      cost 측면에서 기존 모델들의 clip은 extremely dense sampling됨.
 100 이상의 views

    
  


Kinetics-600



  Kinetics-600은 비교적 새로운 데이터이기 때문에 비교 대상이 제한적임. 따라서 이 결과의 주된 목적은 future reference를 제공함에 있음
  Kinetics-600의 validation set이 Kinetics-400의 training set과 겹치기 때문에 400으로 pre-training 하지 않음
  ActivityNet Challenge 2018의 우승 모델(79%), SlowFast, R101 + NL(81.1%)


Experiments: AVA Action Detection
Dataset

  human action에 대한 spatiotemporal localization에 초점
  437개의 영화로부터 데이터 수집
  초당 1프레임에 대해 spatiotemporal label 제공
    
      모든 사람이 bounding box와 action으로 annotation됨
    
  
  training - 211k, validation - 57k
  metric : mAP on 60 classes, using a frame-level IoU threshold of 0.5



Detection architecture

  Faster R-CNN(영상에 적합하게 최소한의 modification 적용)
  SlowFast network를 backbone으로 사용



Training

  상세 내용은 논문에..ㅜ



Inference

  상세 내용은 논문에..ㅜ



Results and Analysis


  Slow-only baseline vs. SlowFast
  Fast pathway를 추가함으로써 5.2mAP 상승 효과
  per-category AP

    
      60개 중 57개의 category에 대해 성능 향상
      성능이 떨어진 3개에 대해, 떨어진 폭이 다른 category의 상승에 비해 작음
    
  
  SlowFast의 다양한 instantiations에 대한 결과

    
      AVA 데이터에 대해 일관된 성능을 보임  robust

    
  


Comparison with state-of-the-art results


  optical flow 사용에 대한 potential benefit을 주목할만 함
  반면 Fast pathway로부터의 성능 향상은 +5.2mAP
    
      optical flow를 사용하는 two-stream method는 computational cost를 두배로 증가시킴
      Fast pathway는 lightweight

    
  
  ground-truth region proposal 분야에서는 35.1 mAP 달성
    
      proposal generation의 성능 향상이 기대됨
    
  
  validation mAP가 26.8인 모델을 train+val 으로 학습한 후 공식 test server에 제출 결과 26.6mAP 달성
 consistency



Visualization

</description>
        <pubDate>Tue, 19 Feb 2019 00:00:00 +0900</pubDate>
        <link>https://youngsankoh.github.io/dlpr/2019/02/19/SlowFast-Networks-for-Video-Recognition/</link>
        <guid isPermaLink="true">https://youngsankoh.github.io/dlpr/2019/02/19/SlowFast-Networks-for-Video-Recognition/</guid>
        
        <category>FAIR</category>
        
        <category>Action_Recognition</category>
        
        
        <category>dlpr</category>
        
      </item>
    
  </channel>
</rss>
