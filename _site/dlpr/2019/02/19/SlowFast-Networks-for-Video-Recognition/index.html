<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Deep Learning paper repo">
	<meta property="og:title" content="SlowFast Networks for Video Recognition" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://youngsankoh.github.io///dlpr/2019/02/19/SlowFast-Networks-for-Video-Recognition/" />
<meta property="og:image" content="https://youngsankoh.github.io//img/bg.jpg">

	

	<meta name="citation_title" content="Project Pages - An Integrated Scientific Blogging Template">


<meta name="citation_author" content="Ahmet Cecen">


<meta name="citation_author" content="David Brough">


<meta name="citation_author" content="Surya R. Kalidindi">



	

	<link rel='shortcut icon' type='image/png' href='/foot.png' />

    <title>SlowFast Networks for Video Recognition - YSlog</title>

    <link rel="canonical" href="https://youngsankoh.github.io/dlpr/2019/02/19/SlowFast-Networks-for-Video-Recognition/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="https://youngsankoh.github.io/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="https://youngsankoh.github.io/css/clean-blog.css">

	<!-- Adjust Colors -->
    <link rel="stylesheet" href="https://youngsankoh.github.io/colorscheme.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="https://youngsankoh.github.io/css/syntax.css">

    <!-- Custom Fonts -->
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
	<!-- Math Jax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript"
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		TeX: { equationNumbers: { autoNumber: "AMS" } }
	});
	</script>

	<!--<script src="http://cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3.min.js" type="text/javascript"></script>-->

    <!-- jQuery -->
	<script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>

	<!-- Bootstrap Core JavaScript -->
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

	<!-- Custom Theme JavaScript -->
	<script src="https://youngsankoh.github.io/js/clean-blog.min.js "></script>

	
<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75881392-1', 'auto');
  ga('send', 'pageview');

</script>


</head>


<body>

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">YSlog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="/">Home</a>
                </li>
				
                <li>
                    <a href="/projects/dlpr/">DLPR</a>
                </li>
                
                
                <li>
                    <a href="/search/">Search</a>
                </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>


    <!-- Post Header -->
<header class="intro-header" style="background-image: url('/img/bg.jpg');">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading" style="padding: 30px 0">
                    <h1>SlowFast Networks for Video Recognition</h1>
                    
                    <h2 class="subheading">2018 Action Classification winner</h2>
                    
                    <span class="meta">Posted by Youngsan on February 19, 2019</span>
                </div>
            </div>
        </div>
    </div>
</header>







	

	

<!-- Also Interesting -->




	



		</div>
		</div>

<!-- Post Content -->
<style>
img {
	display:block;
	max-width:  100%;
	margin-left: auto;
	margin-right: auto;
}

@media only screen and (min-width: 1000px) {
img {
	-moz-transition:-moz-transform 0.5s ease-in; 
	-webkit-transition:-webkit-transform 0.5s ease-in; 
	-o-transition:-o-transform 0.5s ease-in;
}
img:active{
	-moz-transform:scale(1.2); 
	-webkit-transform:scale(1.2);
	-o-transform:scale(1.2);
}
}

@media only screen and (min-width: 1250px) {
img {
	-moz-transition:-moz-transform 0.5s ease-in; 
	-webkit-transition:-webkit-transform 0.5s ease-in; 
	-o-transition:-o-transform 0.5s ease-in;
}
img:active{
	-moz-transform:scale(1.5); 
	-webkit-transform:scale(1.5);
	-o-transform:scale(1.5);
}
}

@media only screen and (min-width: 1500px) {
img {
	-moz-transition:-moz-transform 0.5s ease-in; 
	-webkit-transition:-webkit-transform 0.5s ease-in; 
	-o-transition:-o-transform 0.5s ease-in;
}
img:active{
	-moz-transform:scale(2); 
	-webkit-transform:scale(2);
	-o-transform:scale(2);
}
}
</style>

<article>
    <div id="content" class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">

				<h4 id="key-concept">Key concept</h4>
<ul>
  <li>Fast pathway - high frame rate, motion</li>
  <li>Slow pathway - low frame rate, spatial semantic
<br /><br /><br /></li>
</ul>

<h2 id="introduction">Introduction</h2>
<hr />

<p>network의 motivation</p>
<ul>
  <li>영상에서 categorical spatial semantic은 ‘천천히’ 변함
    <ul>
      <li>‘waving hands’에서 손(categorical semantic)이라는 identity는 변하지 않음</li>
      <li>따라서 categorical semantic을 파악할 때는 low frame rate로 처리해도 됨<br /><br /></li>
    </ul>
  </li>
  <li>반면 motion은 빠르게 변할 수 있음
    <ul>
      <li>high frame rate 필요</li>
    </ul>
  </li>
</ul>

<p><script type="math/tex">\Rightarrow</script> two-pathway SlowFast model을 제안</p>

<p>참고사항)<br /></p>
<ul>
  <li>이 모델은 영장류 시각 시스템의 retinal ganglion cell에서도 부분적으로 영감을 얻음.</li>
  <li>이 cell은 ~80%의 Parvocellular(P-cell), ~15~20%의 Magnocellular(M-cell)로 구성</li>
  <li>M-cell은 spatial detail과 색깔에는 둔감, temporal change에는 민감(high temporal frequency로 작동) (P-cell은 그 반대)
<br /><br /><br /></li>
</ul>

<h2 id="slowfast-networks">SlowFast Networks</h2>
<hr />

<p><img src="https://user-images.githubusercontent.com/22654096/52966598-0eca9780-33eb-11e9-8b7d-d26c9c80d8cd.PNG" alt="Figure1" /></p>

<ul>
  <li>Slow pathway</li>
  <li>Fast pathway</li>
</ul>

<h3 id="slow-pathway">Slow pathway</h3>
<ul>
  <li>video clip을 처리할 수 있는 모든 conv model 사용 가능 e.g., C3D, I3D</li>
  <li>large temporal stride <script type="math/tex">\tau(=16)</script></li>
  <li><script type="math/tex">T</script> : frames sampled by the Slow pathway
<br /><br /></li>
</ul>

<h3 id="fast-pathway">Fast pathway</h3>
<h4 id="high-frame-rate">High frame rate</h4>
<ul>
  <li>small temporal stride <script type="math/tex">\tau/\alpha</script> <script type="math/tex">(\alpha=8)</script>
<br /><br /></li>
</ul>

<h4 id="high-temporal-resolution-features">High temporal resolution features</h4>
<ul>
  <li>temporal downsampling layer 없음 <script type="math/tex">\Rightarrow</script> 항상 <script type="math/tex">\alpha T</script> 차원
    <ul>
      <li>temporal fidelity 유지
<br /><br /></li>
    </ul>
  </li>
</ul>

<h4 id="low-channel-capacity">Low channel capacity</h4>
<ul>
  <li>Slow pathway의 channel 수 대비 <script type="math/tex">\beta</script>배 <script type="math/tex">(\beta=1/8)</script> <script type="math/tex">\Rightarrow</script> lightweight
    <ul>
      <li>Fast pathway은 전체 계산량의 ~20% 차지(M-cell의 비중과 유사)<br /><br /></li>
    </ul>
  </li>
  <li>적은 channel 수로 인해 spatial semantic을 표현하는 능력이 약함.
    <ul>
      <li>spatial modeling ability와 temporal modeling ability 사이의 tradeoff<br /><br /></li>
    </ul>
  </li>
  <li>spatial capacity를 약화시키는 방향의 추가적인 실험
    <ul>
      <li>input spatial resolution 축소</li>
      <li>색깔 정보 제거
<br /><br /></li>
    </ul>
  </li>
</ul>

<h3 id="lateral-connections">Lateral connections</h3>
<ul>
  <li>각 pathway에서 학습한 정보 공유</li>
  <li>매 stage마다 connection, ResNet을 예로 들면 pool1, res2, res3, res4 이후에..</li>
  <li>두 pathway가 처리하는 temporal dimension이 다르기 때문에 일치시켜주는 작업 필요
    <ul>
      <li>Instantiations 절에서 자세히</li>
    </ul>
  </li>
  <li>connection은 unidirectional (fast <script type="math/tex">\rightarrow</script> slow)
    <ul>
      <li>bidirectional도 실험해봤지만 성능면에서 차이 미미
<br /><br /></li>
    </ul>
  </li>
</ul>

<h2 id="instantiations">Instantiations</h2>
<p><img src="https://user-images.githubusercontent.com/22654096/52969842-23139200-33f5-11e9-8081-658d7039b8bb.PNG" alt="Table1" /></p>

<h3 id="slow-pathway-1">Slow pathway</h3>
<p>(위 그림 예)</p>
<ul>
  <li>3D ResNet을 사용, <script type="math/tex">T=4, \tau=16</script></li>
  <li>non-degenerate temporal conv(temporal kernel size&gt;1 인 conv layer, <u>밑줄</u>)를 res4, res5에만 사용
    <ul>
      <li>초기 layer에서의 temporal conv는 정확도에 악영향</li>
      <li><strong>object의 움직임이 빠르고 temporal stride가 크면, spatial receptive field가 충분히 크지 않으면 temporal receptive field 사이에는 correlation이 거의 없다고 판단</strong>
<br /><br /></li>
    </ul>
  </li>
</ul>

<h3 id="fast-pathway-1">Fast pathway</h3>
<ul>
  <li><script type="math/tex">\alpha=8, \beta=1/8</script>  <br /></li>
  <li>higher temporal resolution(green), lower channel capacity(orange)</li>
  <li>non-degenerate conv를 모든 block에 사용
<br /><br /></li>
</ul>

<h3 id="lateral-connections-1">Lateral connections</h3>
<ul>
  <li>각 pathway의 feature shape
    <ul>
      <li>Slow pathway : <script type="math/tex">{\{T, S^2, C\}}</script></li>
      <li>Fast pathway : <script type="math/tex">{\{ \alpha T, S^2, \beta C \}}</script>
<br /><br /></li>
    </ul>
  </li>
  <li>세 가지 fusion 방법
    <ul>
      <li>Time-to-channel : reshape and transpose
<script type="math/tex">{\{ \alpha T, S^2, \beta C \}} \rightarrow {\{T, S^2, \alpha \beta C\}}</script></li>
      <li>Time-strided sampling : sample one out of every <script type="math/tex">\alpha</script> frames
<script type="math/tex">{\{ \alpha T, S^2, \beta C \}} \rightarrow {\{T, S^2, \beta C\}}</script></li>
      <li>Time-strided convolution : 3D conv of <script type="math/tex">5 \times 1^2</script> kernel with stride= <script type="math/tex">\alpha</script>
<script type="math/tex">{\{ \alpha T, S^2, \beta C \}} \rightarrow {\{T, S^2, 2 \beta C\}}</script>
<br /><br /></li>
    </ul>
  </li>
</ul>

<h2 id="experiments-kinetics-action-classification">Experiments: Kinetics Action Classification</h2>
<h4 id="datasets">Datasets</h4>
<ul>
  <li>Kinetics-400</li>
  <li>평가: top-1, top-5 classification accuracy(%), FLOPs
<br /><br /></li>
</ul>

<h4 id="training">Training</h4>
<ul>
  <li>논문 참조
<br /><br /></li>
</ul>

<h4 id="inference">Inference</h4>
<ul>
  <li>논문 참조
<br /><br /></li>
</ul>

<h3 id="results-and-analysis">Results and Analysis</h3>
<p><img src="https://user-images.githubusercontent.com/22654096/52984526-36455280-3433-11e9-8f9a-4a5ff3db5786.PNG" alt="Table2" /></p>

<h4 id="training-from-scratch">Training from scratch</h4>
<ul>
  <li><a href="https://arxiv.org/pdf/1711.07971.pdf">[50]</a></li>
  <li>our recipe : large-scale SGD
<br /><br /></li>
</ul>

<h4 id="individual-pathways">Individual pathways</h4>
<ul>
  <li>no temporal downsampling = temporal reduction factor(t-reduce) of 1</li>
  <li>individual pathway의 성능은 그닥, 다만 GFLOPs를 보면 매우 light함을 알 수 있음</li>
  <li>두 pathway는 합쳐질 때 각자의 special expertise를 보임.(다음 실험)
<br /><br /></li>
</ul>

<h4 id="slowfast-fusion">SlowFast fusion</h4>
<ul>
  <li>SlowFast의 첫번째 row는 naive fusion 사용. 단순 concat</li>
  <li>SlowFast model은 Slow-only 보다 모두 좋은 성능을 보임</li>
  <li>T-conv가 젤 좋음. 따라서 default lateral connection으로 설정
<br /><br /></li>
</ul>

<h4 id="channel-capacity-of-fast-pathway">Channel capacity of Fast pathway</h4>
<ul>
  <li><script type="math/tex">\beta</script> 값에 상관 없이 모두 성능 향상.</li>
  <li>best는 <script type="math/tex">\beta=1/6</script>,  <script type="math/tex">\beta=1/8</script>(default)</li>
  <li><script type="math/tex">\beta=1/32</script>의 경우 GFLOPs가 1.0밖에 안 늘어났지만 1.6%의 성능 향상을 보임
<br /><br /></li>
</ul>

<h4 id="weaker-spatial-inputs-to-fast-pathway">Weaker spatial inputs to Fast pathway</h4>
<ul>
  <li>time difference : 현재 frame - 이전 frame</li>
  <li>모두 Slow-only baseline보다 좋음</li>
  <li>특히 gray-scale의 경우 RGB만큼 좋은 성능을 보이는 동시에 FLOPs도 ~5% 가량 적음
    <ul>
      <li>이 역시 색깔에 둔감한 M-cell과 비슷
<br /><br /></li>
    </ul>
  </li>
</ul>

<h4 id="vs-slowslow">vs. Slow+Slow</h4>
<ul>
  <li>2-Slow ens. : 따로 training된 두 개의 Slow-only 모델을 앙상블<br />
<script type="math/tex">\rightarrow</script> 0.5% 성능 향상, 계산량 x2</li>
  <li>SlowSlow : Fast pathway를 Slow pathway로 교체<br />
<script type="math/tex">\rightarrow</script> overfitting으로 고통받음..
<br /><br /></li>
</ul>

<h4 id="various-slowfast-instantiations">Various SlowFast Instantiations</h4>
<ul>
  <li><script type="math/tex">T \times \tau</script><br />
    <ul>
      <li><script type="math/tex">4 \times 16</script> <br /></li>
      <li><script type="math/tex">8 \times 8</script> <br /></li>
      <li>두 경우 모두 약간의 FLOPs 증가로 성능 향상</li>
    </ul>
  </li>
  <li>나머지는 정확도와 FLOPs 사이의 tradeoff 양상을 보임</li>
  <li>이 tradeoff는 더 최적화될 여지가 남아있으며, 최적의 tradeoff는 application-dependent함
<br /><br /></li>
</ul>

<h4 id="advanced-backbones">Advanced backbones</h4>
<ul>
  <li>ResNet-101 모델과 non-local(NL) version에 대해 추가 실험
    <ul>
      <li>NL은 Slow pathway에만 추가</li>
      <li>R101+NL에서 NL은 res4에만 추가</li>
    </ul>
  </li>
  <li>advanced backbones은 SlowFast concept과는 독립적(orthogonal)
    <ul>
      <li><script type="math/tex">\rightarrow</script> 추가 성능 향상
<br /><br /></li>
    </ul>
  </li>
</ul>

<h4 id="comparison-with-state-of-the-art-results">Comparison with state-of-the-art results</h4>
<p><img src="https://user-images.githubusercontent.com/22654096/53137729-1e9ed300-35c7-11e9-8144-8ba51cf461f8.png" alt="Table3" /></p>

<ul>
  <li>각 논문마다 inference strategy가 다르기 때문에 inference time에 “view”(temporal clip with spatial crop) 개념 도입
    <ul>
      <li>본 논문 예) 10 clips each with 3 spatial crops <script type="math/tex">\rightarrow</script> 30 views</li>
    </ul>
  </li>
  <li>기존 모델(w/wo ImageNet pre-training)보다 성능 좋음. 동시에 inference-time cost도 낮음.
    <ul>
      <li>cost 측면에서 기존 모델들의 clip은 extremely dense sampling됨.<br />
<script type="math/tex">\rightarrow</script> 100 이상의 views
<br /><br /></li>
    </ul>
  </li>
</ul>

<h4 id="kinetics-600">Kinetics-600</h4>
<p><img src="https://user-images.githubusercontent.com/22654096/53139533-42651780-35cd-11e9-83d8-6a865f22dc7c.png" alt="Table4" /></p>

<ul>
  <li>Kinetics-600은 비교적 새로운 데이터이기 때문에 비교 대상이 제한적임. 따라서 이 결과의 주된 목적은 future reference를 제공함에 있음</li>
  <li>Kinetics-600의 validation set이 Kinetics-400의 training set과 겹치기 때문에 400으로 pre-training 하지 않음</li>
  <li>ActivityNet Challenge 2018의 우승 모델(79%), SlowFast, R101 + NL(81.1%)</li>
</ul>

<h2 id="experiments-ava-action-detection">Experiments: AVA Action Detection</h2>
<h4 id="dataset">Dataset</h4>
<ul>
  <li>human action에 대한 spatiotemporal localization에 초점</li>
  <li>437개의 영화로부터 데이터 수집</li>
  <li>초당 1프레임에 대해 spatiotemporal label 제공
    <ul>
      <li>모든 사람이 bounding box와 action으로 annotation됨</li>
    </ul>
  </li>
  <li>training - 211k, validation - 57k</li>
  <li>metric : mAP on 60 classes, using a frame-level IoU threshold of 0.5
<br /><br /></li>
</ul>

<h4 id="detection-architecture">Detection architecture</h4>
<ul>
  <li>Faster R-CNN(영상에 적합하게 최소한의 modification 적용)</li>
  <li>SlowFast network를 backbone으로 사용
<br /><br /></li>
</ul>

<h4 id="training-1">Training</h4>
<ul>
  <li>상세 내용은 논문에..ㅜ
<br /><br /></li>
</ul>

<h4 id="inference-1">Inference</h4>
<ul>
  <li>상세 내용은 논문에..ㅜ
<br /><br /></li>
</ul>

<h3 id="results-and-analysis-1">Results and Analysis</h3>
<p><img src="https://user-images.githubusercontent.com/22654096/53144240-cf649c80-35de-11e9-8529-5d02cf55dccb.png" alt="Table5" /></p>
<ul>
  <li>Slow-only baseline vs. SlowFast</li>
  <li>Fast pathway를 추가함으로써 5.2mAP 상승 효과<br /><br /></li>
  <li>per-category AP
<img src="https://user-images.githubusercontent.com/22654096/53144453-ab558b00-35df-11e9-9971-c9dc939517d1.png" alt="Figure3" />
    <ul>
      <li>60개 중 57개의 category에 대해 성능 향상</li>
      <li>성능이 떨어진 3개에 대해, 떨어진 폭이 다른 category의 상승에 비해 작음<br /><br /></li>
    </ul>
  </li>
  <li>SlowFast의 다양한 instantiations에 대한 결과
<img src="https://user-images.githubusercontent.com/22654096/53144745-d7bdd700-35e0-11e9-9260-e84735b30e79.png" alt="Table6" />
    <ul>
      <li>AVA 데이터에 대해 일관된 성능을 보임 <script type="math/tex">\rightarrow</script> robust
<br /><br /></li>
    </ul>
  </li>
</ul>

<h4 id="comparison-with-state-of-the-art-results-1">Comparison with state-of-the-art results</h4>
<p><img src="https://user-images.githubusercontent.com/22654096/53144852-43a03f80-35e1-11e9-9d91-a941b1ac103a.png" alt="Table7" /></p>
<ul>
  <li>optical flow 사용에 대한 potential benefit을 주목할만 함</li>
  <li>반면 Fast pathway로부터의 성능 향상은 +5.2mAP
    <ul>
      <li>optical flow를 사용하는 two-stream method는 computational cost를 두배로 증가시킴</li>
      <li>Fast pathway는 lightweight
<br /><br /></li>
    </ul>
  </li>
  <li>ground-truth region proposal 분야에서는 35.1 mAP 달성
    <ul>
      <li>proposal generation의 성능 향상이 기대됨<br /><br /></li>
    </ul>
  </li>
  <li>validation mAP가 26.8인 모델을 train+val 으로 학습한 후 공식 test server에 제출 결과 26.6mAP 달성
<script type="math/tex">\rightarrow</script> consistency
<br /><br /></li>
</ul>

<h4 id="visualization">Visualization</h4>
<p><img src="https://user-images.githubusercontent.com/22654096/53145415-79debe80-35e3-11e9-9695-745a344a77c2.png" alt="Figure4" /></p>


                <hr>

                <ul class="pager">
                    
                    
                </ul>

            </div>
        </div>
    </div>
</article>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<hr>

<div class="container" style="padding-right: 50px;padding-left: 50px;">
<div class="row">
<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1" id="disqus_thread">

</div>
</div>
</div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'youngsan';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


<hr>


    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    <!-- <li>
                        <a href="/feed.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li> -->
                    
                    
                    <li>
                        <a href="https://www.instagram.com/donmisekoh">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-instagram fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a href="https://github.com/youngsankoh">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
					<!--  -->
                </ul>
                <p class="copyright text-muted"> Copyright © <a href="http://ahmetcecen.github.io/" target="_blank">Ahmet Cecen</a>,  <a href="http://mined.gatech.edu/" target="_blank">MINED@GaTech</a> and <a href="https://github.com/projectpages" target="_blank">Project Pages</a> 2016. </p>
            </div>
        </div>
    </div>
</footer>

    <script id="dsq-count-scr" src="//youngsan.disqus.com/count.js" async></script>
</body>

</html>
